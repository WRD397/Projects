{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columns with missing values","metadata":{}},{"cell_type":"code","source":"train.columns[train.isna().any()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[:, train.isna().any()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we will be dropping Alley,FireplaceQu,PoolQc,Fence,MiscFeature as they have too many missing values.","metadata":{}},{"cell_type":"markdown","source":"### After Dropping columns","metadata":{}},{"cell_type":"code","source":"train.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True);\ntest.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True);\ntrain.loc[:, train.isna().any()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling missing values","metadata":{}},{"cell_type":"markdown","source":"Both training set and test set have missing values lets handle those.\n1. Missing values in numeric we will handle them by replacing Nan with number which is in proper range of min and max of tht column.(We will also consider average for this)\n2. Missing values in categorical type we will look for any none type(eg: None,No etc.) in the column, if yes replace with them else replace randomly with the unique values in that column.","metadata":{}},{"cell_type":"markdown","source":"#### After handling training set","metadata":{}},{"cell_type":"code","source":"missing = train.columns[train.isna().any()].to_list()\nfor col in missing:\n    if(train[col].dtypes =='float64'):\n        mini= int(train[col].quantile(0.25))\n        maxi= int(train[col].quantile(0.75))\n        listind=train[train[col].isnull()].index.tolist()\n        for i in listind:\n                train.loc[i,col]=random.randint(mini,maxi)\n        train[col]=pd.to_numeric(train[col])\n   \n\n    elif(train[col].dtypes == 'object'):\n        if('True' in str(train[col].str.contains('No').unique().tolist())):\n            train[col].fillna('No',inplace=True)\n        elif('True' in str(train[col].str.contains('None').unique().tolist())):\n            train[col].fillna('None',inplace=True)\n        elif('True' in str(train[col].str.contains('Unf').unique().tolist())):\n            train[col].fillna('Unf',inplace=True)\n        else:\n            listind=train[train[col].isnull()].index.tolist()\n            unique = train[col].unique().tolist()\n            unique=pd.Series(unique).dropna().tolist()\n            for i in listind:\n                train.loc[i,col]=random.choice(unique)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns[train.isna().any()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### After handling test set","metadata":{}},{"cell_type":"code","source":"missing = test.columns[test.isna().any()].to_list()\nfor col in missing:\n    if(test[col].dtypes=='float64'):\n        mini= int(test[col].quantile(0.25))\n        maxi= int(test[col].quantile(0.75))\n        listind=test[test[col].isnull()].index.tolist()\n        for i in listind:\n                test.loc[i,col]=float(random.randint(mini,maxi))\n        test[col]=pd.to_numeric(test[col]) \n        \n    if(test[col].dtypes=='object'):\n        if('True' in str(test[col].str.contains('No').unique().tolist())):\n            test[col].fillna('No',inplace=True)\n        elif('True' in str(test[col].str.contains('None').unique().tolist())):\n            test[col].fillna('None',inplace=True)\n        elif('True' in str(test[col].str.contains('Unf').unique().tolist())):\n            test[col].fillna('Unf',inplace=True)\n        else:\n            listind=test[test[col].isnull()].index.tolist()\n            unique = test[col].unique().tolist()\n            unique=pd.Series(unique).dropna().tolist()\n            for i in listind:\n                test.loc[i,col]=random.choice(unique)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns[test.isna().any()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### train x,y split","metadata":{}},{"cell_type":"code","source":"train_x = train.iloc[:,:-1]\ntrain_y= train.iloc[:,-1]\nprint('Sale price as y')\nprint('----------------')\nprint(train_y.head())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### remove outliers and unnecessary columns","metadata":{}},{"cell_type":"code","source":"train.drop('Id',axis=1,inplace=True)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature encoding","metadata":{}},{"cell_type":"code","source":"objects = train.columns[train.dtypes == 'object'].to_list()\ntrain_x=pd.get_dummies(train_x,columns=objects)\nfor i in objects:\n    cols = train_x.filter(like=i).columns\n    train_x.drop(cols[0],axis=1,inplace=True)\n    \nobjects = test.columns[test.dtypes == 'object'].to_list()\ntest=pd.get_dummies(test,columns=objects)\nfor i in objects:\n    cols = test.filter(like=i).columns\n    test.drop(cols[0],axis=1,inplace=True)\n\nmissing = (list(set(train_x.columns) - set(test.columns)))\ntrain_x.drop(columns = missing,axis = 1,inplace=True)\ntrain_x.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.drop('Id',axis=1,inplace = True)\ntest.drop('Id',axis=1,inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### standardizing","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\ntrain_x = scaler.fit_transform(train_x)\ntest = scaler.fit_transform(test)\n\ntrain_x","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = scaler.fit_transform(train_y.values.reshape(-1,1))\n\ny_new =scaler.inverse_transform(y.reshape(-1,1))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"classifier=Sequential()\nclassifier.add(Dense(output_dim=512,init='uniform',activation='relu',input_dim=217))\n\nclassifier.add(Dense(output_dim=128,init='uniform',activation='relu'))\n\nclassifier.add(Dense(output_dim=128,init='uniform',activation='relu'))\n\nclassifier.add(Dense(output_dim=64,init='uniform',activation='relu'))\n\nclassifier.add(Dense(output_dim=1,init='uniform',activation='relu'))\nclassifier.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_squared_error'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.fit(train_x,y,batch_size=15,epochs=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preidcting test set**","metadata":{}},{"cell_type":"code","source":"y_pred = classifier.predict(test) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\ny_pred = scaler.inverse_transform(y_pred.reshape(-1,1))\nfor i in y_pred:\n    pred.append(i.tolist()[0])\n\npred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\noutput = pd.DataFrame({'Id': new_test.Id,'SalePrice': pred})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}